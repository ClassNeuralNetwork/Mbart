{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6ZvppVm0fL6",
        "outputId": "b307040d-03db-4a39-d4ae-44ccab2bb898"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Geração de texto com ajustes de temperatura e top-k sampling ###\n",
            "\n",
            "Temperature: 1.0, Top-k: 20\n",
            "Generated Text: Sejam os pilotos das vossas histórias e voem o mais alto possível\n",
            "\n",
            "Temperature: 1.0, Top-k: 40\n",
            "Generated Text: Sejam os pilotos das vossas histórias e voam o mais alto possível\n",
            "\n",
            "Temperature: 1.0, Top-k: 80\n",
            "Generated Text: Sejam os pilotos das vossas histórias e voem o mais alto possível\n",
            "\n",
            "Temperature: 2.5, Top-k: 20\n",
            "Generated Text: Sejam o piloto das vossas histórias\n",
            "\n",
            "Temperature: 2.5, Top-k: 40\n",
            "Generated Text: Sês o piloto das suas histórias e voa quanto mais alto possível\n",
            "\n",
            "Temperature: 2.5, Top-k: 80\n",
            "Generated Text: Sejam o piloto das vossas histórias\n",
            "\n",
            "Temperature: 3.5, Top-k: 20\n",
            "Generated Text: Serem o piloto das vossas histórias e voarem tanto quanto puderam\n",
            "\n",
            "Temperature: 3.5, Top-k: 40\n",
            "Generated Text: Sejam o piloto da vossa história e voassem mais alto que puderem\n",
            "\n",
            "Temperature: 3.5, Top-k: 80\n",
            "Generated Text: Sejam quem vos arrebate as histórias e voam\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "\n",
        "model_name = \"facebook/mbart-large-50-one-to-many-mmt\"\n",
        "\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang=\"en_XX\")\n",
        "\n",
        "article_en = \"Be the pilot of your stories and fly as high as you can\"\n",
        "\n",
        "inputs = tokenizer(article_en, return_tensors=\"pt\")\n",
        "\n",
        "# Definir valores de temperatura e top_k para experimentação\n",
        "temperature_values = [1.0, 2.5, 3.5]  # Valores de temperatura\n",
        "top_k_values = [20, 40, 80]          # Valores de top-k\n",
        "\n",
        "print(\"### Geração de texto com ajustes de temperatura e top-k sampling ###\\n\")\n",
        "\n",
        "# Loop para variar os valores de temperatura e top-k\n",
        "for temp in temperature_values:\n",
        "    for top_k in top_k_values:\n",
        "        # Gerar o texto traduzido com parâmetros ajustados\n",
        "        generated_ids = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_length=200,\n",
        "            do_sample=True,       # Ativar a amostragem para top-k sampling\n",
        "            temperature=temp,      # Controlar a aleatoriedade\n",
        "            top_k=top_k,           # Limitar as palavras candidatas ao top-k\n",
        "            forced_bos_token_id=tokenizer.lang_code_to_id[\"pt_XX\"]  # Código para português\n",
        "        )\n",
        "\n",
        "        # Decodificar e exibir o texto gerado\n",
        "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        print(f\"Temperature: {temp}, Top-k: {top_k}\\nGenerated Text: {generated_text}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvO7_i37FrY9",
        "outputId": "26d5033f-988d-41b2-fe7c-f96dfe0edb39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed 33: Sejam os pilotos das vossas histórias e voem o mais alto possível\n",
            "Seed 143: Sejam os pilotos das vossas histórias e voem o mais alto possível\n",
            "Seed 456: Sejam os pilotos das vossas histórias e voem o mais alto possível\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "\n",
        "model_name = \"facebook/mbart-large-50-one-to-many-mmt\"\n",
        "\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang=\"en_XX\")\n",
        "\n",
        "article_en = \"Be the pilot of your stories and fly as high as you can\"\n",
        "\n",
        "inputs = tokenizer(article_en, return_tensors=\"pt\")\n",
        "\n",
        "seeds = [33, 143, 456]\n",
        "generated_texts = []\n",
        "\n",
        "# Para cada seed, definir a seed e gerar o texto traduzido\n",
        "for seed in seeds:\n",
        "    # Definir a seed\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Traduzir do inglês para português com parâmetros ajustados\n",
        "    generated_tokens = model.generate(\n",
        "        **inputs,\n",
        "        forced_bos_token_id=tokenizer.lang_code_to_id[\"pt_XX\"],  # Código para português\n",
        "        do_sample=True,             # Habilita amostragem para variabilidade\n",
        "        temperature=1.0,            # Ajuste de temperatura para controlar a criatividade\n",
        "        max_length=50               # Limite máximo de tokens na saída\n",
        "    )\n",
        "\n",
        "    # Decodificar a tradução e adicionar aos resultados\n",
        "    generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "    generated_texts.append((seed, generated_text))\n",
        "\n",
        "# Exibir os resultados para cada seed\n",
        "for seed, text in generated_texts:\n",
        "    print(f\"Seed {seed}: {text}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
